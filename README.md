# XYScanNet: An Interpretable State Space Model for Perceptual Image Deblurring

<p align="center">
  <img src="https://img.shields.io/badge/CVPR--NTIRE%202025-Accepted-brightgreen.svg">
  <img src="https://img.shields.io/badge/arXiv-2412.10338-blue">
</p>

[![arXiv](https://img.shields.io/badge/arXiv-2412.10338-b31b1b.svg)](https://arxiv.org/abs/2412.10338)

Official implementation of our **CVPR Workshop NTIRE 2025** accepted paper:

> **XYScanNet: An Interpretable State Space Model for Perceptual Image Deblurring**  
> *Hanzhou Liu, Chengkai Liu, Jiacong Xu, Peng Jiang, Mi Lu*  
> [arXiv 2412.10338](https://arxiv.org/abs/2412.10338)

---

## 📝 Abstract

We propose **XYScanNet**, a novel interpretable state space model tailored for image deblurring. By decoupling the horizontal and vertical motion components using independent state transition branches, XYScanNet captures direction-specific blur more effectively and with greater interpretability. We further enhance it with learnable observation decoders and diagonal residual fusion to improve reconstruction quality without compromising clarity. Extensive experiments on synthetic and real-world datasets demonstrate that XYScanNet achieves strong performance in both distortion and perceptual metrics, while offering deeper insights into the motion dynamics behind image degradation.

---

## 🚀 Highlights

- ✅ **Accepted at CVPR 2025 - NTIRE Workshop**
- 📐 Direction-aware motion modeling using independent X- and Y-scan branches.
- 🔬 Interpretability through disentangled motion estimations.
- 📈 Competitive results on GoPro, HIDE, and RealBlur datasets.

---

## 📁 Code & Dataset

Coming soon! Stay tuned.

---

## 📖 Citation

If you find our work useful, please consider citing us:

```bibtex
@article{liu2024xyscannet,
  title={XYScanNet: An Interpretable State Space Model for Perceptual Image Deblurring},
  author={Liu, Hanzhou and Liu, Chengkai and Xu, Jiacong and Jiang, Peng and Lu, Mi},
  journal={arXiv preprint arXiv:2412.10338},
  year={2024}
}
